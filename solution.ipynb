{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ探索概要\n",
    "- 純粋なカテゴリカル変数でかつ、カテゴリ数の少ない`item_condition_id`, `shipping`はダミー変数化\n",
    "- textで記載されている特徴量である`name`, `item_description`、 `category_name`は基本的には下記の中から選択\n",
    "    - CountVectorizer or TfidfVectorizer\n",
    "    - Cleaningの有無\n",
    "    - ngramの範囲\n",
    "    - binary表現有無\n",
    "\n",
    "\n",
    "|特徴量 |EDA概要| 方針 |  \n",
    "|------|---------------|----|\n",
    "| item_condition_id | 5値カテゴリカル変数。 | ダミー変数化 | \n",
    "| shipping | 2値のカテゴリカル変数。 | ダミー変数化 |\n",
    "| name |1,225,273カテゴリあり、最大17語、7語とかが多そう| tf-idf| \n",
    "| category_name| 1,287カテゴリあるのでone-hotはつらそう。<br> general_cat, subcat1, subcat2へ分解。それぞれがせいぜい3wordくらい　| それぞれをcountvetor化| \n",
    "| brand_name | 4809カテゴリあり、最大7語、だいたいが1、2語　| Label binarize | \n",
    "| item_description| 最大245語の文章。 <br> Not described yetなどの定型文もある。 |  tf-idf | \n",
    "| name2 | `name`+`brand_name` <br> 1stで使われていた特徴量。 | Tfidf(max_features=100000, token_pattern='\\w+') | \n",
    "| text | `item_description`+`name+category_name` <br> 1stで使われていた特徴量。 | Tfidf(max_features=100000, token_pattern='\\w+', ngram_range=(1, 2)) | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from contextlib import contextmanager\n",
    "from functools import partial\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import time\n",
    "from typing import List, Dict\n",
    "\n",
    "import keras as ks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as Tfidf\n",
    "from sklearn.pipeline import make_pipeline, make_union, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "DATADIR = Path('./input')\n",
    "\n",
    "tr_path = DATADIR / 'train.tsv'\n",
    "test_path = DATADIR / 'test.tsv'\n",
    "\n",
    "train_cols = ['name', 'item_condition_id', 'category_name', 'brand_name','shipping', 'item_description', 'price']\n",
    "test_cols =  ['name', 'item_condition_id', 'category_name', 'brand_name','shipping', 'item_description']\n",
    "\n",
    "train = pd.read_csv(tr_path, sep='\\t', usecols=train_cols, parse_dates=True, keep_date_col=True)\n",
    "test = pd.read_csv(test_path, sep='\\t', usecols=test_cols, parse_dates=True, keep_date_col=True)\n",
    "\n",
    "y_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 処理時間の管理\n",
    "- contextmanagerを使うことでwith句で囲ってやるだけで、処理時間が表示されるようになって便利。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} done in {:.0f} s\".format(name, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴量作成\n",
    "### preprocessでの特徴量作成 \n",
    "- テキストに関連する特徴量を工夫している、\n",
    "- `name`と`brand_name`, `item_description`と`name`と`cateogry_name`を混ぜてtextを作っている。\n",
    "- `*アスタリスク`はapply関数を呼び出しているときにunpackingの機能として適用されいる。`_split_cat()`で各サンプルごとにリストが返され、これをアンパックして各要素を取り出した上で、zipすることでサンプル横断で各要素(general_cat, subcat_1, subcat_2)を得ている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"preprocessing for whole dataset\n",
    "    Args:\n",
    "        df (pd.DataFrame): original whole data from .tsv file.\n",
    "    Returns:\n",
    "        pd.DataFrame: data that text concatinated features added.\n",
    "    \"\"\"\n",
    "    df['name2'] = df['name'].fillna('') + ' ' + df['brand_name'].fillna('')\n",
    "    df['text'] = (df['item_description'].fillna('') + ' ' + df['name'] + ' ' + df['category_name'].fillna(''))\n",
    "    df['general_cat'], df['subcat_1'], df['subcat_2'] = zip(*df['category_name'].apply(lambda x: _split_cat(x)))\n",
    "\n",
    "    return df[['name', 'text', 'shipping', 'item_condition_id', 'name2']]\n",
    "\n",
    "def _split_cat(text):\n",
    "    try: return text.split(\"/\")\n",
    "    except: return (\"No Label\", \"No Label\", \"No Label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### on_fieldによるpipeline管理\n",
    "make_pipeline, FunctionTransformer, itemgetterを使って特徴量作成をコンパクトにまとめている。\n",
    "```python\n",
    "def on_field(feature: str) -> Pipeline:\n",
    "    return make_pipeline(FunctionTransformer(itemgetter(feature), validate=False))\n",
    "```\n",
    "- make_pipeline(): 処理のパイプラインを作成する\n",
    "- FunctionTransformer(): すべての要素に関数を適用する\n",
    "- itemgetter(f): 今回の使われ方だと指定されたfieldのpd.Seriesを返す。e.g. itemgetter('name') -> df['name']。下記と等価。\n",
    "\n",
    "```python\n",
    "def itemgetter(item):\n",
    "    def g(obj):\n",
    "        return obj[item]\n",
    "    return g\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_field(feature: str, *vectorizer) -> Pipeline:\n",
    "    \"\"\"make a pipeline for vectorization with the specified feature.\n",
    "    Args:\n",
    "        feature (str): column name for the preprocessing target feature.\n",
    "        vectorizer : functions for vectorization.\n",
    "    Return: A pipeline for preprocessing.\n",
    "    \"\"\"\n",
    "    return make_pipeline(FunctionTransformer(itemgetter(feature), validate=False), *vectorizer)\n",
    "\n",
    "def to_records(df: pd.DataFrame) -> List[Dict]:\n",
    "    \"\"\"convert the DataFrame to a dictionary.\n",
    "    Args:\n",
    "        df (pd.DataFrame): data.\n",
    "    Returns:\n",
    "        List of dictionaries. Keys are column names, values are values of cell.\n",
    "    \"\"\"\n",
    "    return df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make_unionによる特徴量ベクトル化の効率化\n",
    "make_unionで[FeatureUnion](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion)クラスのvectorizerインスタンスを作っている。このインスタンスは後に`fit_transform`メソッドが呼ばれてtransformersによって前処理された結果がhstackされたnp.ndarray(n_samples, sum_n_components)を返している。特徴量作成に便利そう。さらっとあるn_jobsがnotebook上でもおしゃれに並列化をかませてくれる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = make_union(\n",
    "    on_field('name', Tfidf(max_features=100000, token_pattern='\\w+')),\n",
    "    on_field('text', Tfidf(max_features=100000, token_pattern='\\w+', ngram_range=(1, 2))),\n",
    "    on_field(['shipping', 'item_condition_id'], FunctionTransformer(to_records, validate=False), DictVectorizer()),n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Valid分割 \n",
    "- KFoldでn_split=20としているが、next()でiteretorを1回しか呼び出していないので実質20分の1だけvalidにsubsamplingしてきただけ？\n",
    "\n",
    "### 説明変数`price`の前処理\n",
    "- log(y+1)変換:y_trainをポアソン回帰的にじゃなくてlog(y+1)変換したものに対してfitするようにしている。後の学習時に`mean squared error`をfitしに行っているので評価関数的に整合性取れている。\n",
    "- スケールを平均0、分散1に変更している。2stage制なので2ndのテストデータがどのようなものが来るかわからないから？あとで読む。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train valid data split done in 1 s\n",
      "X_train: (1407577, 200002) of float32\n",
      "process train done in 214 s\n",
      "X_valid: (74084, 200002) of float32\n",
      "process valid done in 46 s\n"
     ]
    }
   ],
   "source": [
    "with timer('train valid data split'):\n",
    "    train = train[train['price'] > 0].reset_index(drop=True) # reset_indexないとilocで変な値取ってくる\n",
    "    cv = KFold(n_splits=20, shuffle=True, random_state=42)\n",
    "    train_ids, valid_ids = next(cv.split(train))\n",
    "    train, valid = train.iloc[train_ids], train.iloc[valid_ids]\n",
    "\n",
    "with timer('process train'):  \n",
    "    y_train = y_scaler.fit_transform(np.log1p(train['price'].values.reshape(-1, 1)))\n",
    "    X_train = vectorizer.fit_transform(preprocess(train)).astype(np.float32)\n",
    "    print('X_train: {} of {}'.format(X_train.shape, X_train.dtype))\n",
    "    del train\n",
    "    \n",
    "with timer('process valid'):\n",
    "    y_valid = valid['price']\n",
    "    X_valid = vectorizer.transform(preprocess(valid)).astype(np.float32)\n",
    "    print('X_valid: {} of {}'.format(X_valid.shape, X_valid.dtype))\n",
    "    del valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "### モデル\n",
    "- モデル自体は`fit_predict`でtensorflowでの4層パーセプトロン。\n",
    "- vectorizerがsparse matrixを返してくるので`sparse=True`とされている。\n",
    "- batch_sizeが学習が進むと徐々に大きくなる。最初小さいバッチサイズで進めて、さくさく勾配を下させて、除々に速度を犠牲にしてデータ全体に最適化するようにしている。学習率大→小へと進めるのと似た意味合い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict(xs, y_train) -> np.ndarray:\n",
    "    X_train, X_test = xs\n",
    "    config = tf.ConfigProto(\n",
    "        intra_op_parallelism_threads=1, use_per_session_threads=1, inter_op_parallelism_threads=1)\n",
    "    with tf.Session(graph=tf.Graph(), config=config) as sess, timer('fit_predict'):\n",
    "        ks.backend.set_session(sess)\n",
    "        model_in = ks.Input(shape=(X_train.shape[1],), dtype='float32', sparse=True)\n",
    "        out = ks.layers.Dense(192, activation='relu')(model_in)\n",
    "        out = ks.layers.Dense(64, activation='relu')(out)\n",
    "        out = ks.layers.Dense(64, activation='relu')(out)\n",
    "        out = ks.layers.Dense(1)(out)\n",
    "        model = ks.Model(model_in, out)\n",
    "        model.compile(loss='mean_squared_error', optimizer=ks.optimizers.Adam(lr=3e-3))\n",
    "        for i in range(3):\n",
    "            with timer('epoch {}'.format(i+1)):\n",
    "                model.fit(x=X_train, y=y_train, batch_size=2**(11 + i), epochs=1, verbose=0)\n",
    "        return model.predict(X_test)[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4つのNNモデルのアンサンブル＆並列学習\n",
    "- NNを活用する際は確率的勾配法を活用していることから同じモデルでもアンサンブルした方が良い？\n",
    "- np.boolとすることでzeroとnon-zeroで2値化されたデータを作った。意味合い的にはtf-idfからbinary CountVectorizeしていることになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "epoch 1 done in 511 s\n",
      "epoch 1 done in 556 s\n",
      "epoch 1 done in 556 s\n",
      "epoch 1 done in 557 s\n",
      "epoch 2 done in 297 s\n",
      "epoch 2 done in 273 s\n",
      "epoch 2 done in 295 s\n",
      "epoch 2 done in 295 s\n",
      "epoch 3 done in 171 s\n",
      "fit_predict done in 985 s\n",
      "epoch 3 done in 160 s\n",
      "fit_predict done in 996 s\n",
      "epoch 3 done in 165 s\n",
      "epoch 3 done in 166 s\n",
      "fit_predict done in 1024 s\n",
      "fit_predict done in 1025 s\n",
      "Valid RMSLE: 0.3936\n"
     ]
    }
   ],
   "source": [
    "with ThreadPool(processes=4) as pool:\n",
    "    Xb_train, Xb_valid = [x.astype(np.bool).astype(np.float32) for x in [X_train, X_valid]]\n",
    "    xs = [[Xb_train, Xb_valid], [X_train, X_valid]] * 2\n",
    "    y_pred = np.mean(pool.map(partial(fit_predict, y_train=y_train), xs), axis=0)\n",
    "    y_pred = np.expm1(y_scaler.inverse_transform(y_pred.reshape(-1, 1))[:, 0])\n",
    "    print('Valid RMSLE: {:.4f}'.format(np.sqrt(mean_squared_log_error(y_valid, y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
